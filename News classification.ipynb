{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC News classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to classify different news articles into their corresponding topics in a unsupervised way, matrix factorization specifically. Here I used non-negative matrix method. You can find the dataset at:\n",
    "https://www.kaggle.com/competitions/learn-ai-bbc/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>857</td>\n",
       "      <td>double eviction from big brother model caprice...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>325</td>\n",
       "      <td>dj double act revamp chart show dj duo jk and ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>1590</td>\n",
       "      <td>weak dollar hits reuters revenues at media gro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>1587</td>\n",
       "      <td>apple ipod family expands market apple has exp...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>538</td>\n",
       "      <td>santy worm makes unwelcome visit thousands of ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ArticleId                                               Text  \\\n",
       "0          1833  worldcom ex-boss launches defence lawyers defe...   \n",
       "1           154  german business confidence slides german busin...   \n",
       "2          1101  bbc poll indicates economic gloom citizens in ...   \n",
       "3          1976  lifestyle  governs mobile choice  faster  bett...   \n",
       "4           917  enron bosses in $168m payout eighteen former e...   \n",
       "...         ...                                                ...   \n",
       "1485        857  double eviction from big brother model caprice...   \n",
       "1486        325  dj double act revamp chart show dj duo jk and ...   \n",
       "1487       1590  weak dollar hits reuters revenues at media gro...   \n",
       "1488       1587  apple ipod family expands market apple has exp...   \n",
       "1489        538  santy worm makes unwelcome visit thousands of ...   \n",
       "\n",
       "           Category  \n",
       "0          business  \n",
       "1          business  \n",
       "2          business  \n",
       "3              tech  \n",
       "4          business  \n",
       "...             ...  \n",
       "1485  entertainment  \n",
       "1486  entertainment  \n",
       "1487       business  \n",
       "1488           tech  \n",
       "1489           tech  \n",
       "\n",
       "[1490 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('BBC News Train.csv')\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check for null values across the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleId    0\n",
       "Text         0\n",
       "Category     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.duplicated(subset=['Text', 'Category']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_cleaned = data_train.drop_duplicates(subset = ['Text', 'Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>857</td>\n",
       "      <td>double eviction from big brother model caprice...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>325</td>\n",
       "      <td>dj double act revamp chart show dj duo jk and ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>1590</td>\n",
       "      <td>weak dollar hits reuters revenues at media gro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>1587</td>\n",
       "      <td>apple ipod family expands market apple has exp...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>538</td>\n",
       "      <td>santy worm makes unwelcome visit thousands of ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ArticleId                                               Text  \\\n",
       "0          1833  worldcom ex-boss launches defence lawyers defe...   \n",
       "1           154  german business confidence slides german busin...   \n",
       "2          1101  bbc poll indicates economic gloom citizens in ...   \n",
       "3          1976  lifestyle  governs mobile choice  faster  bett...   \n",
       "4           917  enron bosses in $168m payout eighteen former e...   \n",
       "...         ...                                                ...   \n",
       "1485        857  double eviction from big brother model caprice...   \n",
       "1486        325  dj double act revamp chart show dj duo jk and ...   \n",
       "1487       1590  weak dollar hits reuters revenues at media gro...   \n",
       "1488       1587  apple ipod family expands market apple has exp...   \n",
       "1489        538  santy worm makes unwelcome visit thousands of ...   \n",
       "\n",
       "           Category  \n",
       "0          business  \n",
       "1          business  \n",
       "2          business  \n",
       "3              tech  \n",
       "4          business  \n",
       "...             ...  \n",
       "1485  entertainment  \n",
       "1486  entertainment  \n",
       "1487       business  \n",
       "1488           tech  \n",
       "1489           tech  \n",
       "\n",
       "[1440 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_cleaned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Building fractions of the dataset for model tuning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1066</td>\n",
       "      <td>bellamy under new fire newcastle boss graeme s...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>1570</td>\n",
       "      <td>schools to take part in mock poll record numbe...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>998</td>\n",
       "      <td>rap boss arrested over drug find rap mogul mar...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>96</td>\n",
       "      <td>dent continues adelaide progress american tayl...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>355</td>\n",
       "      <td>neeson in bid to revive theatre hollywood film...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>883</td>\n",
       "      <td>web logs aid disaster recovery some of the mos...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>362</td>\n",
       "      <td>us woman sues over cartridges a us woman is su...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>32</td>\n",
       "      <td>firefox browser takes on microsoft microsoft s...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>491</td>\n",
       "      <td>giggs handed wales leading role ryan giggs wil...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>514</td>\n",
       "      <td>brazil buy boosts belgium s inbev belgian brew...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ArticleId                                               Text  \\\n",
       "169        1066  bellamy under new fire newcastle boss graeme s...   \n",
       "612        1570  schools to take part in mock poll record numbe...   \n",
       "555         998  rap boss arrested over drug find rap mogul mar...   \n",
       "65           96  dent continues adelaide progress american tayl...   \n",
       "638         355  neeson in bid to revive theatre hollywood film...   \n",
       "...         ...                                                ...   \n",
       "1477        883  web logs aid disaster recovery some of the mos...   \n",
       "641         362  us woman sues over cartridges a us woman is su...   \n",
       "702          32  firefox browser takes on microsoft microsoft s...   \n",
       "689         491  giggs handed wales leading role ryan giggs wil...   \n",
       "261         514  brazil buy boosts belgium s inbev belgian brew...   \n",
       "\n",
       "           Category  \n",
       "169           sport  \n",
       "612        politics  \n",
       "555   entertainment  \n",
       "65            sport  \n",
       "638   entertainment  \n",
       "...             ...  \n",
       "1477           tech  \n",
       "641            tech  \n",
       "702            tech  \n",
       "689           sport  \n",
       "261        business  \n",
       "\n",
       "[144 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a random 10% sample\n",
    "data_train_cleaned_10 = data_train_cleaned.sample(frac=0.1, random_state=42)\n",
    "data_train_cleaned_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Word embedding (turning text into matrix) using TF-IDF which based on the word occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.02045097, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.02409841, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = data_train_cleaned ['Text'].tolist()\n",
    "vectorizer = TfidfVectorizer() # max_features=500, model tuning \n",
    "X_train = vectorizer.fit_transform(texts)\n",
    "# vectorizer.get_feature_names()\n",
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Understand the number of topics in true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'tech', 'politics', 'sport', 'entertainment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_cleaned['Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Building NMF model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 5  # choose based on your dataset\n",
    "nmf = NMF(n_components=n_topics)\n",
    "W = nmf.fit_transform(X_train)   # [n_docs × n_topics]\n",
    "H = nmf.components_        # [n_topics × n_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04526626, 0.04636159, 0.00727894, 0.        , 0.00158877],\n",
       "       [0.15364127, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.11399654, 0.02019131, 0.01160904, 0.01235422, 0.03983508],\n",
       "       ...,\n",
       "       [0.14628174, 0.        , 0.00342779, 0.        , 0.        ],\n",
       "       [0.04154235, 0.        , 0.        , 0.00849862, 0.19583675],\n",
       "       [0.0260935 , 0.        , 0.00672267, 0.00296844, 0.11141211]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.96461410e-04, 1.03204129e-01, 8.45023149e-05, ...,\n",
       "        4.91740295e-04, 2.77877674e-04, 0.00000000e+00],\n",
       "       [7.55868529e-04, 4.04072338e-02, 1.96064181e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.17545869e-04, 0.00000000e+00, 2.57060494e-04, ...,\n",
       "        9.47998586e-03, 3.59726027e-04, 5.29288447e-03],\n",
       "       [5.72065023e-04, 2.20706730e-02, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.42998951e-03, 1.54304372e-03],\n",
       "       [1.08916014e-03, 5.84294698e-02, 5.21710673e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Determine labels in each document based on highest number of association. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 4, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = np.argmax(W, axis=1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trying to match the perdicted labels with true labels by looking at a few matching labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            business\n",
       "1            business\n",
       "2            business\n",
       "3                tech\n",
       "4            business\n",
       "            ...      \n",
       "1485    entertainment\n",
       "1486    entertainment\n",
       "1487         business\n",
       "1488             tech\n",
       "1489             tech\n",
       "Name: Category, Length: 1440, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_cleaned ['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di = {0: 'business', 1: 'politics', 2: 'sport', 3: 'entertainment', 4: 'tech'}\n",
    "new = []\n",
    "for label in predicted_labels:\n",
    "    new.append(di[label])\n",
    "predicted_train_new = np.array(new)\n",
    "acc_train = accuracy_score(data_train_cleaned['Category'], predicted_train_new)\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Confusion matrix + hungarian or brute force method for finding accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[306,  12,   2,   0,  15],\n",
       "       [ 14,   5,  20, 188,  36],\n",
       "       [ 23, 207,   2,   0,  34],\n",
       "       [  2,   0, 337,   2,   1],\n",
       "       [  7,   1,   2,   2, 222]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true labels (e.g., from your Category column)\n",
    "true_labels = data_train_cleaned [\"Category\"].astype(\"category\").cat.codes.values\n",
    "\n",
    "# Create a confusion matrix between predicted and true\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# Apply Hungarian algorithm to find best label mapping\n",
    "row_ind, col_ind = linear_sum_assignment(-conf_matrix)\n",
    "\n",
    "# Map predicted labels to best-aligned true labels\n",
    "label_mapping = dict(zip(col_ind, row_ind))\n",
    "mapped_preds = np.array([label_mapping[label] for label in predicted_labels])\n",
    "\n",
    "# Now accuracy is valid!\n",
    "acc = accuracy_score(true_labels, mapped_preds)\n",
    "print(f\"Adjusted Accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Adjusted ran and normalized mutual for finding accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.729, NMI: 0.710\n"
     ]
    }
   ],
   "source": [
    "ari = adjusted_rand_score(data_train_cleaned ['Category'], predicted_labels)\n",
    "nmi = normalized_mutual_info_score(data_train_cleaned ['Category'], predicted_labels)\n",
    "\n",
    "print(f\"ARI: {ari:.3f}, NMI: {nmi:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Display top 10 highest weighted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: the | in | of | to | and | its | us | growth | said | economy\n",
      "Topic 1: the | mr | to | he | labour | election | blair | of | and | party\n",
      "Topic 2: the | to | and | in | he | we | his | of | but | england\n",
      "Topic 3: the | film | best | and | awards | in | of | for | award | her\n",
      "Topic 4: the | to | of | and | that | is | are | people | it | in\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[-10:][::-1]]\n",
    "    print(f\"Topic {topic_idx}: {' | '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised model building and training using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets (80% train, 20% test)\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    data_train_cleaned['Text'],           # Input texts\n",
    "    data_train_cleaned['Category'],          # Labels\n",
    "    test_size=0.2,   # 20% of data for testing\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Convert text to TF-IDF (after splitting to avoid data leakage)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_text)  # Fit on training data only\n",
    "X_test = vectorizer.transform(X_test_text)        # Transform test data\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. I did not apply the model to the test set as I noticed that the articles aren't correctly labeled from the solution file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Model tuning and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With different feature parameters, we are able to find out that the max feature and accuracy are in a linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Max_feature | Accuracy |\n",
    "| :-: | :-: |\n",
    "| 500 | 0.772 |\n",
    "|1000 | 0.828 |\n",
    "|5000 | 0.869 |\n",
    "| 10000 | 0.875 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With different amout of training data, looks like 30% fraction of the data gives the highest accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Data fraction | Accuracy \n",
    "| :-: | :-: |\n",
    "| 10% | 0.868 |\n",
    "| 30% | 0.884 |\n",
    "| 50% | 0.875 |\n",
    "| 70% | 0.869 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Supervised vs Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Accuracy |\n",
    "| :-: | :-: |\n",
    "| NMF | 0.875 - 0.884 |\n",
    "| Logistic | 0.97 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To build SVD models for comparison\n",
    "2. To split the training set into test set and apply to nmf model too\n",
    "3. Use grid search to find the best hyperparameters for supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF: https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUNGARIAN: https://www.geeksforgeeks.org/hungarian-algorithm-assignment-problem-set-1-introduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADJUSTED RAND SCORE: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalized_mutual_info_score: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
